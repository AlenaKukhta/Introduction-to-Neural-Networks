{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e770984",
   "metadata": {},
   "source": [
    "# Практическое задание к уроку 7. Детектирование объектов\n",
    "\n",
    "<ol>\n",
    "    <li>Сделайте краткий обзор какой-нибудь научной работы посвященной тому или иному алгоритму для object detection, который не рассматривался на уроке. Проведите анализ: Чем отличается выбранная вами на рассмотрение архитектура нейронной сети от других архитектур? В чем плюсы и минусы данной архитектуры? Какие могут возникнуть трудности при применении данной архитектуры на практике?\n",
    "    </li> \n",
    "    <li>Запустите детектор (ssdMobile_v2 или faster_rcnn, или любой другой детектор) для своей картинки и попробуйте найти 10 объектов, 100 объектов.\n",
    "    </li>\n",
    "    <li>* Ссылка на репозиторий с полным кодом для обучения ssd нейросети - https://github.com/sergeyveneckiy/ssd-tensorflow. Попробуйте улучшить точность ее работы и напишите отчет, что вы пробовали изменить в ее параметрах и как это отражалось на процессе обучения нейронной сети. \n",
    "        Обратите внимание! Мин. сист. требования для запуска данного проекта - это минимум 8 Gb ОЗУ. Если у вас недостаточно мощности компьютера, то вы можете просто изучить содержимое исходного кода и датасета данного проекта.</li>\n",
    "\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e50b08",
   "metadata": {},
   "source": [
    "#### Microsoft COCO: Common Objects in Context (https://arxiv.org/pdf/1405.0312.pdf)\n",
    "\n",
    "Создание бесконечно сложных сетей без подготовленных данных не целесообразно, поскольку невозможно достич сколько-нибудь нового функционала. Например ImageNet, в свое время, позволил сделать прорывы как в классификации объектов, так и в исследованиях обнаружения. Новый набор данных MS COCO способен решать следующие исследовательские задачи: обнаружение non-iconic представлений объектов, контекстуальных рассуждений между объектами и точной 2D-локализации объектов.\n",
    "\n",
    "Набор данных Microsoft Common Objects in COntext (MS COCO) содержит 91 категорию общих объектов, причем 82 из них имеют более 5000 помеченных экземпляров. В общей сложности набор данных содержит 2 500 000 помеченных экземпляров в 328 000 изображениях. При этом набор данных содержит не отдельные изображения с категориями, как cifar-10, cifar-100, изображения содержат несколько объектов, на фоне сложных повседневных сцен.\n",
    "\n",
    "Поскольку обнаружение многих объектов, таких как солнцезащитные очки, мобильные телефоны или стулья, сильно зависит от контекстуальной информации, важно, чтобы наборы данных обнаружения содержали объекты в их естественной среде обитания. В наборе данных MS COCO изображения, богатые контекстуальной информацией. В других наборах данных классы распределены неравномерно (стена: 20 213, окно: 16 080, стул: 7 971, лодка: 349, самолет: 179, торшер: 276), в MS COCO гарантируют, что каждая категория объектов имеет значительное число экземпляров. Чтобы обеспечить практический сбор значительного числа экземпляров в каждой категории, MS COCO ограничили метками категорий, которые обычно используются людьми при описании объектов (собака, стул, человек)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937f053b",
   "metadata": {},
   "source": [
    "#### You Only Look Once (YOLO)\n",
    "\n",
    "https://medium.com/zylapp/review-of-deep-learning-algorithms-for-object-detection-c1f3d437b852 \n",
    "https://arxiv.org/pdf/1506.02640.pdf \n",
    "http://pjreddie.com/yolo/ \n",
    "\n",
    "Модель YOLO основана на GoogLeNet и яляется более легкой версией - она имеет всего 9 сверточных слоев вместо 24. Простота модели YOLO позволяет делать прогнозы в реальном времени. \n",
    "\n",
    "YOLO накладывает строгие пространственные ограничения на предсказания ограничивающего прямоугольника, поскольку каждая ячейка сетки предсказывает только два прямоугольника и может иметь только один класс. Это пространственное ограничение ограничивает количество близлежащих объектов, которые может предсказать модель. Модель борется с небольшими объектами, которые появляются группами, например стаями птиц. \n",
    "\n",
    "YOLO допускает гораздо меньше фоновых ошибок, чем Fast R-CNN. Используя YOLO для устранения фоновых обнаружений из Fast R-CNN, получаем значительное повышение производительности. Для каждого ограничивающего прямоугольника, который предсказывает R-CNN, проверяем, предсказывает ли YOLO аналогичный прямоугольник. Если это произойдет, дадим этому прогнозу повышение на основе вероятности, предсказанной YOLO, и наложения между двумя блоками. Лучшая модель Fast R-CNN достигает 71,8% в тестовом наборе VOC 2007. В сочетании с YOLO mAР увеличивается на 3,2% до 75,0%. \n",
    "\n",
    "Увеличение от YOLO - это не просто побочный продукт ансамбля моделей, так как объединение различных версий Fast R-CNN дает мало пользы. Скорее, именно потому, что YOLO допускает различные виды ошибок во время тестирования, алгоритм настолько эффективен для повышения производительности Fast R-CNN. К сожалению, эта комбинация не выигрывает от скорости YOLO, поскольку происходит запуск каждой модели отдельно, а затем объединяются результаты. Однако, поскольку алгоритм YOLO очень быстр, он не добавляет значительного времени вычислений по сравнению с Fast R-CNN. \n",
    "\n",
    "YOLO - унифицированная модель для обнаружения объектов. Она проста в изготовлении и может быть обучена прямо на полных изображениях. В отличие от подходов, основанных на классификаторе, YOLO обучается работе с функцией потерь, которая напрямую соответствует эффективности обнаружения, и вся модель обучается совместно. YOLO использует новейшие технологии обнаружения объектов в реальном времени. YOLO также хорошо подходит для новых сфер, что делает его идеальным для приложений, в которых требуется быстрое и надежное обнаружение объектов. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f41e0e",
   "metadata": {},
   "source": [
    "#### Архитектуры семейства YOLOv5\n",
    "\n",
    "Было рассмотрено семейство алгоритмов YOLOv5 (YOLOv5x, YOLOv5l, YOLOv5m, YOLOv5s)\n",
    "\n",
    "***Плюсы***\n",
    "\n",
    "Чрезвычайно просты в использовании для разработчика, внедряющего технологии компьютерного зрения в приложение.\n",
    "Встроенный механизм \"умной\" аугментации Mosaic augmentation, решающий проблему пропуска мелких объектов другими сетями.\n",
    "Быстрая скорость обучения по сравнению с аналогичными сетями YOLOv4.\n",
    "Минусы\n",
    "\n",
    "Нет научной статьи сопровождающий релиз данной архитектуры.\n",
    "Уступает YOLOv4 по многим важным параметрам.\n",
    "\n",
    "\n",
    "***Трудности применения***\n",
    "\n",
    "Одна из основных задач, поставленная автором данной архитетктуры, как я понял из одного интервью с ним, была как раз в том, чтобы максимально упростить использование данной сети для решения любых связанных задач. Жалоб на трудности применения в комментариях к статьям и на GitHub не обнаружил.\n",
    "\n",
    "***YOLO***\n",
    "\n",
    "***Критика алгоритма в профессиональном сообществе***\n",
    "\n",
    "Автор данного алгоритма подвергся критике со стороны разработчика YOLOv4 и других специалистов области машинного обучения. Основная причина связана с выбором имени для алгоритма, подразумевающем развитие сети YOLOv4. На деле же, этот алгоритм скорее находится рядом c YOLOv4 и не имеет черт YOLOv4, которых нет у YOLOv3. То есть это по сути другая ветвь развития YOLOv3, которая больше ориентирована на скорость работы и простоту, но по точности уступает YOLOv4. Также автор не выпустил вместе с релизом научную работу, описывающую подробно все его доработки. А также сравнения YOLOv5 автором с YOLOv4 были непрезентабельны из-за разных датасетов и условий проведения тестов.\n",
    "\n",
    "***Ссылки на источники***\n",
    "\n",
    "https://blog.roboflow.com/yolov4-versus-yolov5/\n",
    "\n",
    "https://github.com/AlexeyAB/darknet/issues/5920\n",
    "\n",
    "https://blog.roboflow.com/yolov5-improvements-and-evaluation/\n",
    "\n",
    "https://github.com/AlexeyAB/darknet/issues/6067"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589cdda9",
   "metadata": {},
   "source": [
    "#### Алгоритм Deeplab\n",
    "\n",
    "Deeplab (Semantic Image Segmentation with Deep Convolutional Nets) – глубокая сверточная нейронная сеть для решения задач сегментации объектов на изображении, построенная на основе VGG-16. Отличительными особенностями архитектуры являются:\n",
    "\n",
    "Свертка с фильтрами с повышенной дискретизацией (atrous convolution) (заменяются последние слои понижения дискретизации сети VGG-16). Данная свертка позволяет явно контролировать разрешение, при котором характеристики отклика вычисляются в глубоких сверточных нейронных сетях и позволяет эффективно расширять поле зрения фильтров, чтобы включать больший контекст без увеличения количества параметров или объема вычислений.\n",
    "\n",
    "Atrous Spatial Pyramid Pooling (ASPP) - методика \"пространственного пирамидального объединения\". Данная методика позволяет боротся с проблемой сегментации объектов в разных масштабах. Данная методика использует фильтры разной частоты на разных параллельных сверточных слоях для повышения поля зрения фильтров.\n",
    "\n",
    "Улучшение локализации границ объектов с использованием вероятностных графических моделей (conditional random field - CRF). Достигается это путем объединения ответов на конечном уровне сети с полностью подключенным условным случайным полем (CRF), которое показано как качественно, так и количественно для улучшения характеристик локализации (Фильтр Гаусса).\n",
    "\n",
    "***Плюсы архитектуры:*** хорошая сигментация объектов разного масштаба, лучшее определение границ объекта, сегментация в большой плотности объектов.\n",
    "\n",
    "***Минусы архитектуры:*** небольшая скорость обучения, невозможность \"захватить\" тонкие границы объектов (велосипед, стул).\n",
    "\n",
    "***Источники:***\n",
    "\n",
    "https://arxiv.org/pdf/1606.00915.pdf\n",
    "https://www.machinelearningmastery.ru/the-evolution-of-deeplab-for-semantic-segmentation-95082b025571/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a15d83",
   "metadata": {},
   "source": [
    "#### Статья Mingxing Tan, Ruoming Pang and Quoc V. Le. EfficientDet: Scalable and Efficient Object Detection\n",
    "   \n",
    "В этой статье авторы изучили архитектуру сетей для детекции объектов и предложили взвешенную двунаправленную feature network (BiFPN) и метод масштабирования для повышения точности и эффективности алгоритма. На основе этих оптимизаций разработано новое семейство детекторов под названием EfficientDet, которое добивается большей точности и эффективности, чем предшествующие алгоритмы с учетом ресурсных ограничений (оперативная память и число операций). В частности, масштабируемый EfficientDet достигает наилучшей точности с гораздо меньшим количеством параметров и FLOP, чем предыдущие модели.\n",
    "\n",
    "В последнее время алгоритмы object detection достигли большой точности, но при этом содержат очень много параметров и, соответственно, требуют многих вычислений. Это ограничивает применение данных алгоритмов там, где есть ограничения по времени отклика и аппаратной мощности. Также есть требования масштабировать конкретную архитектуру для применения ее от мобильных устройств до self driving car. \n",
    "\n",
    "EfficientDet - масштабируемая архитектура object detection. Дает высокую точность и производительность с выбором ресурсных ограничений из широкого диапазона (3B to 300 BFLOP) благодаря использованию:\n",
    "    - BiFPN - взвешенная двунаправленная пирамидальная сеть признаков (является развитием FPN);\n",
    "    - метод масштабирования, который позволяет изменять разрешение, глубину и ширину всех составных частей сети: encoder, decoder, BiFPN, box prediction network, class prediction network;\n",
    "    - EfficientNet B0 - B6 в качестве encoder, предтренированные на ImageNet.\n",
    "\n",
    "При одинаковой точности EfficientDet требует в 28 раз меньше операций, чем YOLO v3 и в 30 - чем Retina Net, а также может после незначительных изменений применяться для решения задач сегментации."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c812da75",
   "metadata": {},
   "source": [
    "#### Выводы при создании нейронных сетей\n",
    "\n",
    "- Не всегда лучшая сеть для классификации объектов будет лучшей в качестве backbone для сети, используемой для обнаружения объектов\n",
    "- Использование weights обученных с фичами, которые увеличили точность в классификации, может негативно сказаться на точности детектора (на некоторых сетях)\n",
    "- Не все фичи заявленные в различных исследованиях улучшают точность сети\n",
    "- Не все фичи совместимы между собой и некоторые комбинации фич зачастую уменьшают точность сети при использовании совместно\n",
    "- Не всегда сеть с более низким BFLOPS будет быстрее, даже если BFLOPS меньше в десятки раз\n",
    "- Сети для обнаружения объектов требуют более высокого разрешения сети для обнаружения множества объектов разного размера и их точного местоположения, это требует более высокого receptive field для покрытия увеличенного разрешения сети, а значит требуется больше слоев со stride=2 и/или conv3x3, и больший размер weights (filters) для запоминания большего числа деталей объектов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635e6ef8",
   "metadata": {},
   "source": [
    "#### 2. Запустите детектор (ssdMobile_v2 или faster_rcnn, или любой другой детектор) для своей картинки и попробуйте найти 10 объектов, 100 объектов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6736d25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:72.5% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML, Image\n",
    "display(HTML('<style>.container { width:72.5% !important; }</style>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee049588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'darknet' already exists and is not an empty directory.\r\n"
     ]
    }
   ],
   "source": [
    "# clone darknet repo\n",
    "!git clone https://github.com/AlexeyAB/darknet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bb0db68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/alenakukhta/Downloads/GB/Введение в нейронные сети/7_Детектирование объектов/darknet\n",
      "sed: 1: \"Makefile\": invalid command code M\n",
      "sed: 1: \"Makefile\": invalid command code M\n",
      "sed: 1: \"Makefile\": invalid command code M\n",
      "sed: 1: \"Makefile\": invalid command code M\n"
     ]
    }
   ],
   "source": [
    "# change makefile to have GPU and OPENCV enabled\n",
    "%cd darknet\n",
    "!sed -i 's/OPENCV=0/OPENCV=1/' Makefile\n",
    "!sed -i 's/GPU=0/GPU=1/' Makefile\n",
    "!sed -i 's/CUDNN=0/CUDNN=1/' Makefile\n",
    "!sed -i 's/CUDNN_HALF=0/CUDNN_HALF=1/' Makefile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686f3a88",
   "metadata": {},
   "source": [
    "#### YOLOv4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "148a40b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-02-01 21:06:51--  https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.weights\n",
      "Распознаётся github.com (github.com)… 140.82.121.3\n",
      "Подключение к github.com (github.com)|140.82.121.3|:443... соединение установлено.\n",
      "HTTP-запрос отправлен. Ожидание ответа… 302 Found\n",
      "Адрес: https://objects.githubusercontent.com/github-production-release-asset-2e65be/75388965/ba4b6380-889c-11ea-9751-f994f5961796?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220201%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220201T180651Z&X-Amz-Expires=300&X-Amz-Signature=411a9b14264d582b7686c8a55ad270a3913bfbf5f6768dae5678b375585b622d&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=75388965&response-content-disposition=attachment%3B%20filename%3Dyolov4.weights&response-content-type=application%2Foctet-stream [переход]\n",
      "--2022-02-01 21:06:51--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/75388965/ba4b6380-889c-11ea-9751-f994f5961796?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220201%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220201T180651Z&X-Amz-Expires=300&X-Amz-Signature=411a9b14264d582b7686c8a55ad270a3913bfbf5f6768dae5678b375585b622d&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=75388965&response-content-disposition=attachment%3B%20filename%3Dyolov4.weights&response-content-type=application%2Foctet-stream\n",
      "Распознаётся objects.githubusercontent.com (objects.githubusercontent.com)… 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Подключение к objects.githubusercontent.com (objects.githubusercontent.com)|185.199.110.133|:443... соединение установлено.\n",
      "HTTP-запрос отправлен. Ожидание ответа… 200 OK\n",
      "Длина: 257717640 (246M) [application/octet-stream]\n",
      "Сохранение в: «yolov4.weights.2»\n",
      "\n",
      "yolov4.weights.2    100%[===================>] 245,78M  5,61MB/s    за 47s     \n",
      "\n",
      "2022-02-01 21:07:39 (5,18 MB/s) - «yolov4.weights.2» сохранён [257717640/257717640]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c78c97b",
   "metadata": {},
   "source": [
    "Вспомогательные функции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16745c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define helper functions\n",
    "def imShow(path):\n",
    "  import cv2\n",
    "  import matplotlib.pyplot as plt\n",
    "  %matplotlib inline\n",
    "\n",
    "  image = cv2.imread(path)\n",
    "  height, width = image.shape[:2]\n",
    "  resized_image = cv2.resize(image,(3*width, 3*height), interpolation = cv2.INTER_CUBIC)\n",
    "\n",
    "  fig = plt.gcf()\n",
    "  fig.set_size_inches(18, 10)\n",
    "  plt.axis(\"off\")\n",
    "  plt.imshow(cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB))\n",
    "  plt.show()\n",
    "\n",
    "# use this to upload files\n",
    "def upload():\n",
    "  from google.colab import files\n",
    "  uploaded = files.upload() \n",
    "  for name, data in uploaded.items():\n",
    "    with open(name, 'wb') as f:\n",
    "      f.write(data)\n",
    "      print ('saved file', name)\n",
    "\n",
    "# use this to download a file  \n",
    "def download(path):\n",
    "  from google.colab import files\n",
    "  files.download(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2dc20a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imagenet.shortnames.list\n",
      "dog.jpg\n",
      "imagenet.labels.list\n",
      "coco9k.map\n",
      "coco.names\n",
      "9k.tree\n",
      "scream.jpg\n",
      "voc.names\n",
      "eagle.jpg\n",
      "goal.txt\n",
      "openimages.names\n",
      "person.jpg\n",
      "labels\n",
      "giraffe.jpg\n",
      "horses.jpg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for f in os.listdir(\"/Users/alenakukhta/Downloads/GB/darknet/data/\"):\n",
    "\tprint(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e75a2d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda activate environment-name\n",
    "# conda init </Users/alenakukhta/Downloads/GB/darknet/data/person.jpg>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6d83307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no such file or directory: ./Users/alenakukhta/Downloads/GB/darknet/data/person.jpg\r\n"
     ]
    }
   ],
   "source": [
    "# run darknet detection on test images\n",
    "!./Users/alenakukhta/Downloads/GB/darknet/data/person.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33079534",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@1.955] global /Users/runner/work/opencv-python/opencv-python/opencv/modules/imgcodecs/src/loadsave.cpp (239) findDecoder imread_('predictions.jpg'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/64/g6jhnpm16ql1vgrrbrbd_yd00000gn/T/ipykernel_10251/311980275.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# show image using our helper function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimShow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'predictions.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/64/g6jhnpm16ql1vgrrbrbd_yd00000gn/T/ipykernel_10251/3246389094.py\u001b[0m in \u001b[0;36mimShow\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0mresized_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINTER_CUBIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# show image using our helper function\n",
    "imShow('predictions.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1ab4ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ed6ec1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90961cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
